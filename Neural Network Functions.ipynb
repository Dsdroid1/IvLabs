{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Some variables used\n",
    "n_x=No.of features of input X\n",
    "m=No.of training examples in X\n",
    "n_y=No.of nodes in last layer(No.of classes for multiclass system)\n",
    "\n",
    "X(Input matrix) should be of dimensions (n_x,m)\n",
    "Y(True output matrix) should be of dimensions (n_y,m)\n",
    "If it is a multiclass system,then n_y corresponds to each case of it being in a class\n",
    "so this matrix will have 1 for the class that example belongs to and zero otherwise(such is used for loss fn)\n",
    "\n",
    "layer_dims-It is an array containing the no.of nodes in each layer,starting from input layer\n",
    "to output layer.So it is of the form [n_x,...,n_y]\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    #Input:dimensions of layers\n",
    "    #Output:dictionary containing weights and bais for them\n",
    "    parameters=dict()\n",
    "    for i in range(len(layer_dims)-1):\n",
    "        parameters[\"W\"+str(i+1)]=np.random.randn(layer_dims[i+1],layer_dims[i])*0.01\n",
    "        parameters[\"b\"+str(i+1)]=np.zeros((layer_dims[i+1],1))\n",
    "        assert(parameters[\"W\"+str(i+1)]).shape==(layer_dims[i+1],layer_dims[i])\n",
    "        assert(parameters[\"b\"+str(i+1)]).shape==(layer_dims[i+1],1)\n",
    "        \n",
    "    return parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''The following part will contain functions required for FORWARD PROPAGATION'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function is called by forward_act,which calculates the activation of a layer\n",
    "def forward_prop(A_prev,W,b):\n",
    "    #Calculates Z\n",
    "    #Inputs:activations of previous layer,current layer weights and bias\n",
    "    #Output:Z\n",
    "    Z=np.dot(W,A_prev)+b\n",
    "    assert(Z.shape==(W.shape[0],A_prev.shape[1]))\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is further called by L_layer_forward,which completes the forward pass part of the cycle\n",
    "def forward_act(A_prev,W,b,activation):\n",
    "    #Input -Type of activation,activations of previous layer,current layer weights and bias\n",
    "    #Output-Activation of current layer,Z(to be cached,in L_layer_forward)\n",
    "    Z=forward_prop(A_prev,W,b)\n",
    "    if activation=='sigmoid':\n",
    "        A=1/(1+np.exp(-Z))\n",
    "    elif activation=='relu':\n",
    "        A=np.maximum(0,Z)\n",
    "    elif activation=='tanh':\n",
    "        A=(np.exp(Z)-np.exp(-Z))/(np.exp(Z)+np.exp(-Z))\n",
    "    elif activation=='softmax':\n",
    "        A=np.exp(Z)/np.sum(np.exp(Z),axis=0,keepdims=True)\n",
    "        \n",
    "    assert(Z.shape==(W.shape[0],A_prev.shape[1]))    \n",
    "    return A ,Z   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_forward(X,parameters):\n",
    "   #Inputs:X-the input matrix\n",
    "   #       parameters-dictionary containing weights and biases\n",
    "   #Output:Activations of last layer,Cache containing Z,Cache containing all prev activations(including X,\n",
    "   #     for looping purpose) \n",
    "   #This function contains code for activation as relu for all layers except for last,which has softmax for multiclass\n",
    "   #classification,so it can be changed if required\n",
    "    l=int(len(parameters)/2)\n",
    "    A_prev=X\n",
    "    Z_caches=[]\n",
    "    A_caches=[]\n",
    "    A_caches.append(X)\n",
    "    #caches.append(X)\n",
    "    for i in range(l):\n",
    "        if i==l-1:#Usually for last layer,different activation is used,here softmax  \n",
    "            AL,cache=forward_act(A_prev,parameters[\"W\"+str(i+1)],parameters[\"b\"+str(i+1)],'softmax')\n",
    "        else:#For other layers,relu,can be changed according to requirement\n",
    "            A,cache=forward_act(A_prev,parameters[\"W\"+str(i+1)],parameters[\"b\"+str(i+1)],'relu')\n",
    "            A_prev=A\n",
    "            A_caches.append(A_prev)\n",
    "        Z_caches.append(cache)\n",
    "    A_caches.append(AL)\n",
    "    \n",
    "   # assert(AL.shape==(n_y,m))\n",
    "    return AL,Z_caches,A_caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''The next part contains the cost function\n",
    "Different function should be selected according to the model requirements\n",
    "****Regression****\n",
    "J=-1/(2m)*np.power((AL-Y),2)\n",
    "****Binary classification****\n",
    "J=-1/m*np.sum(Y*np.log(AL)+(1-Y)*(np.log(1-AL)))\n",
    "****Multiclass classification****\n",
    "J=-1/m*np.sum(np.sum(np.log(AL)*Y,axis=0,keepdims=True),axis=1,keepdims=True)\n",
    "*********************************\n",
    "(Note:The concept behind classifactioncost function is same,that the only term remaining in the loss\n",
    "function is the only one which is the true class(value of y=1 at that pt) and our log of probability\n",
    "that it belongs to that class.To minise error our prob. should be as high as possible)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(AL,Y):\n",
    "    #***************************************\n",
    "    '''Choose the cost function from above para,according to requirements'''\n",
    "    #***************************************\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''The following part contains BACKWARD PROPAGATION'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function just calculates the dZ(L) part,knowing the dA(L)\n",
    "#It is basically for g'(Z) \n",
    "#It is called by grad\n",
    "def backwards(dA,Z,activation):\n",
    "    #Inputs:The dA for L layer,Z of that layer,type of activation that layer went through in forward pass\n",
    "    #Output:dZ for that layer\n",
    "    #'*' represents element wise multiplication\n",
    "    if activation=='relu':\n",
    "        temp=(Z>0)\n",
    "        dZ=dA*temp\n",
    "    elif activation=='softmax':\n",
    "        dZ=dA*(np.exp(Z)/np.sum(np.exp(Z),axis=0,keepdims=True))*(1-np.exp(Z)/np.sum(np.exp(Z),axis=0,keepdims=True))\n",
    "        #d/dx of softmax=softmax(x)*(1-softmax(x))\n",
    "    elif activation=='sigmoid':\n",
    "        dZ=dA*1/(1+np.exp(-Z))*(1-1/(1+np.exp(-Z)))\n",
    "    elif activation=='tanh':\n",
    "        dZ=dA*(1-np.power((np.exp(Z)-np.exp(-Z))/(np.exp(Z)+np.exp(-Z)),2))\n",
    "        \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(A_caches,Z_caches,parameters,AL,Y):\n",
    "    #Inputs:caches containing A,Z(made during FORWARD PASS),set of parameters(dict()),activation of last layer\n",
    "    #      (also present in A_cache,needs optimisation),Y-the true output matrix\n",
    "    #Internal Inputs:Activation type of last layer,Activation type of all other layers\n",
    "    #Output:Dictionary containing gradients of Z,W,b\n",
    "    grads=dict()\n",
    "    l=int(len(parameters)/2)#Parameter has W and b,and we iterate through a W,b pair at once\n",
    "    grads[\"dZ\"+str(l)]=AL-Y#Is same for all 3 cost functions mentioned here\n",
    "    for i in reversed(range(l)):\n",
    "        grads[\"dW\"+str(i+1)]=1/m*np.dot(grads[\"dZ\"+str(i+1)],A_caches[i].T)\n",
    "        grads[\"db\"+str(i+1)]=1/m*np.sum(grads[\"dZ\"+str(i+1)],axis=1,keepdims=True)\n",
    "        if i==0:break#We dont want to calculate dJ/dX(input)\n",
    "        dA=np.dot(parameters[\"W\"+str(i+1)].T,grads[\"dZ\"+str(i+1)])\n",
    "        if i==l-1:\n",
    "            grads[\"dZ\"+str(i)]=backwards(dA,Z_caches[i-1],'softmax')\n",
    "        else:\n",
    "            grads[\"dZ\"+str(i)]=backwards(dA,Z_caches[i-1],'relu')   \n",
    "        \n",
    "    return grads    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_paramters(parameters,grads,learning_rate):\n",
    "    #Does the work of updating weights and biases\n",
    "    #Inputs:gradients dictionary,parameter dictionary,learning_rate-alpha(Try to plot J vs num_iter to get best alpha)\n",
    "    l=int(len(parameters)/2)\n",
    "    for i in range(l):\n",
    "        parameters[\"W\"+str(i+1)]-=learning_rate*grads[\"dW\"+str(i+1)]\n",
    "        parameters[\"b\"+str(i+1)]-=learning_rate*grads[\"db\"+str(i+1)]\n",
    "        \n",
    "    return parameters    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''The next part contains a whole model trainer,which utilizes the above declared functions'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_model(X,Y,layer_dims,num_iter=1500,print_cost=False):\n",
    "    #Inputs:X-The input matrix\n",
    "    #       Y-The true output matrix\n",
    "    #       layer_dims-As specified above\n",
    "    #       num_iter-No.of iterations for gradient descent\n",
    "    #       print_cost(bool)-To tell if plotting J vs num_iter has to be done\n",
    "    #Output:Effective parameters after gradient descent\n",
    "    costs=[]\n",
    "    #******************************\n",
    "    #define the learning rate\n",
    "    learning_rate=0.01\n",
    "    #***************************\n",
    "    parameters=initialize_parameters(layer_dims)\n",
    "    for i in range(num_iter):\n",
    "        AL,Z_caches,A_caches=L_layer_forward(X,parameters)\n",
    "        J=cost(AL,Y)\n",
    "        grads=grad(A_caches,Z_caches,parameters,AL,Y)\n",
    "        parameters=update_paramters(parameters,grads,learning_rate)\n",
    "        # Print the J every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration \"+str(i)+\":\"+str(J))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(J)\n",
    "       \n",
    "    # plot the J vs num_iter\n",
    "\n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "        \n",
    "    \n",
    "    return parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
